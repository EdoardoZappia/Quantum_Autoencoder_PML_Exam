{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/24j7jy9s6sb2xm_dc84kdhm00000gn/T/ipykernel_79551/2401498332.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/edoardozappia/Downloads/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = df[df.Class == 1]\n",
    "normal = df[df.Class == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(['Time'], axis=1)\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227451, 29)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train = X_train[X_train.Class == 0]\n",
    "X_train = X_train.drop(['Class'], axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "#y_test = X_test['Class']\n",
    "#X_test = X_test.drop(['Class'], axis=1)\n",
    "\n",
    "X_train = X_train.values\n",
    "#X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "class_0 = X_test[X_test.Class == 0]\n",
    "class_1 = X_test[X_test.Class == 1]\n",
    "\n",
    "total_size = len(X_test)\n",
    "desired_size_class_1 = int(total_size * 0.4)\n",
    "desired_size_class_0 = total_size - desired_size_class_1\n",
    "\n",
    "class_1_sampled = class_1.sample(n=desired_size_class_1, replace=True, random_state=42)\n",
    "class_0_sampled = class_0.sample(n=desired_size_class_0, replace=True, random_state=42)\n",
    "\n",
    "balanced_data = pd.concat([class_0_sampled, class_1_sampled])\n",
    "\n",
    "X_test = balanced_data.drop('Class', axis=1)\n",
    "y_test = balanced_data['Class']\n",
    "\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34178\n",
      "22784\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(y_test == 0))\n",
    "print(np.count_nonzero(y_test == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "input_shape=(29,)\n",
    "print(input_dim)\n",
    "original_dim = input_dim  # Esempio di dimensione dell'input originale\n",
    "intermediary_dims = [20, 10 ,8]\n",
    "latent_dim = 2  # Esempio di dimensione dello spazio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:47:58.148118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/edoardozappia/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 1.2974\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - loss: 1.2965\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - loss: 1.2956\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step - loss: 1.2946\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 1.2936\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 1.2926\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - loss: 1.2915\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - loss: 1.2903\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - loss: 1.2891\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - loss: 1.2879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x145c51c40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "\n",
    "# Definizione della classe MultivariateNormalTriLLayer\n",
    "class MultivariateNormalTriLLayer(tfkl.Layer):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(MultivariateNormalTriLLayer, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.untransformed_scale_tril = self.add_weight(name='untransformed_scale_tril',\n",
    "                                                        shape=(self.latent_dim * (self.latent_dim + 1) // 2,),\n",
    "                                                        initializer='random_normal',\n",
    "                                                        trainable=True)\n",
    "        super(MultivariateNormalTriLLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        loc = inputs[..., :self.latent_dim]\n",
    "        scale_tril = tfp.math.fill_triangular(self.untransformed_scale_tril)\n",
    "        dist = tfd.MultivariateNormalTriL(loc=loc, scale_tril=scale_tril)\n",
    "        sample = dist.sample()\n",
    "        return sample  # Return the sample, not the distribution object\n",
    "\n",
    "# Definizione della funzione dense_layers\n",
    "def dense_layers(intermediary_dims):\n",
    "    return tfk.Sequential([\n",
    "        tfkl.Dense(units, activation='relu') for units in intermediary_dims\n",
    "    ])\n",
    "\n",
    "# Definizione del decoder come una classe Layer\n",
    "class Decoder(tfkl.Layer):\n",
    "    def __init__(self, original_dim, intermediary_dims, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.decoder_hidden = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation='relu') for units in intermediary_dims\n",
    "        ] + [\n",
    "            tf.keras.layers.Dense(original_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.decoder_hidden(inputs)\n",
    "\n",
    "# Definizione del modello VAE\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, original_dim, intermediary_dims, latent_dim, prior, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediary_dims = intermediary_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior = prior\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(original_dim, intermediary_dims)\n",
    "\n",
    "    def build_encoder(self):\n",
    "        x = dense_layers(self.intermediary_dims)(self.encoder_inputs)\n",
    "        params = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(self.latent_dim), activation=None)(x)\n",
    "        z = MultivariateNormalTriLLayer(self.latent_dim)(params)\n",
    "        return tf.keras.Model(self.encoder_inputs, z, name='encoder')\n",
    "\n",
    "    def kl_divergence(self, distribution_a):\n",
    "        distribution_b = self.prior\n",
    "        return tfpl.KLDivergenceAddLoss(\n",
    "            tfpl.MultivariateNormalTriL(),\n",
    "            distribution_b,\n",
    "            weight=1.0\n",
    "        )(distribution_a)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_sample = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z_sample)\n",
    "        return reconstructed\n",
    "\n",
    "# Definizione dei parametri\n",
    "input_dim = X_train.shape[1]\n",
    "original_dim = input_dim\n",
    "intermediary_dims = [20, 10, 8]\n",
    "latent_dim = 2\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "\n",
    "# Creazione dell'istanza del modello VAE\n",
    "vae = VAE(original_dim, intermediary_dims, latent_dim, prior)\n",
    "\n",
    "# Compilazione del modello specificando la loss\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(), loss=reconstruction_loss)\n",
    "\n",
    "# Allenamento del modello\n",
    "vae.fit(X_train, X_train, epochs=10, batch_size=2000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponendo che vae sia il tuo modello VAE addestrato e X_test e y_test siano i tuoi dati di test\n",
    "#X_test_encoded = vae.encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della perdita di ricostruzione su X_test\n",
    "#reconstructed = vae.decoder(vae.encoder(X_test)).numpy()\n",
    "#reconstruction_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(X_test, reconstructed), axis=1)\n",
    "\n",
    "# Definizione di una soglia arbitraria (può essere ottimizzata)\n",
    "#threshold = 2.5  # Esempio di soglia, da ottimizzare\n",
    "\n",
    "# Creazione di un array binario per le previsioni basate sulla soglia\n",
    "#predictions = reconstruction_loss > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche con soglia arbitraria:\n",
      "AUC: 0.9529\n",
      "Accuracy: 0.9116\n",
      "Precision: 0.9204\n",
      "Recall: 0.8527\n",
      "F1-score: 0.8853\n",
      "TN: 32498, FP: 1680, FN: 3355, TP: 19429\n",
      "\n",
      "ROC Curve Optimization:\n",
      "Optimal Threshold (ROC): inf\n",
      "AUC (ROC): 0.9529\n",
      "Accuracy (ROC): 0.6000\n",
      "Precision (ROC): 0.0000\n",
      "Recall (ROC): 0.0000\n",
      "F1-score (ROC): 0.0000\n",
      "TN (ROC): 34178, FP (ROC): 0, FN (ROC): 22784, TP (ROC): 0\n",
      "\n",
      "Precision-Recall Curve Optimization:\n",
      "Optimal Threshold (PR): 2.3579\n",
      "AUC (PR): 0.9529\n",
      "Accuracy (PR): 0.9264\n",
      "Precision (PR): 0.9171\n",
      "Recall (PR): 0.8971\n",
      "F1-score (PR): 0.9070\n",
      "TN (PR): 32331, FP (PR): 1847, FN (PR): 2345, TP (PR): 20439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edoardozappia/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calcolo della perdita di ricostruzione su X_test\n",
    "reconstructed = vae.decoder(vae.encoder(X_test)).numpy()\n",
    "reconstruction_loss = np.mean(np.square(X_test - reconstructed), axis=1)\n",
    "\n",
    "# Definizione di una soglia arbitraria (può essere ottimizzata)\n",
    "threshold = 2.5  # Esempio di soglia, da ottimizzare\n",
    "\n",
    "# Creazione di un array binario per le previsioni basate sulla soglia\n",
    "predictions = (reconstruction_loss > threshold).astype(np.int32)\n",
    "\n",
    "# Definisci la funzione per calcolare le metriche in base al threshold\n",
    "def calculate_metrics(y_test, predictions, reconstruction_loss):\n",
    "    # Calcolo delle metriche\n",
    "    auc = roc_auc_score(y_test, reconstruction_loss)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return auc, accuracy, precision, recall, f1, tn, fp, fn, tp\n",
    "\n",
    "# Calcolo delle metriche per la soglia arbitraria\n",
    "auc, accuracy, precision, recall, f1, tn, fp, fn, tp = calculate_metrics(y_test, predictions, reconstruction_loss)\n",
    "\n",
    "print(f'Metriche con soglia arbitraria:')\n",
    "print(f'AUC: {auc:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
    "print()\n",
    "\n",
    "# Ottimizzazione del threshold utilizzando la curva ROC\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, reconstruction_loss)\n",
    "roc_auc_scores = [roc_auc_score(y_test, reconstruction_loss) for threshold in thresholds_roc]\n",
    "optimal_threshold_roc = thresholds_roc[np.argmax(roc_auc_scores)]\n",
    "\n",
    "# Applica il threshold ottimizzato dalla curva ROC\n",
    "predictions_roc = (reconstruction_loss > optimal_threshold_roc).astype(np.int32)\n",
    "\n",
    "# Calcola le metriche utilizzando l'optimal_threshold_roc\n",
    "auc_roc, accuracy_roc, precision_roc, recall_roc, f1_roc, tn_roc, fp_roc, fn_roc, tp_roc = calculate_metrics(y_test, predictions_roc, reconstruction_loss)\n",
    "\n",
    "print(f'ROC Curve Optimization:')\n",
    "print(f'Optimal Threshold (ROC): {optimal_threshold_roc:.4f}')\n",
    "print(f'AUC (ROC): {auc_roc:.4f}')\n",
    "print(f'Accuracy (ROC): {accuracy_roc:.4f}')\n",
    "print(f'Precision (ROC): {precision_roc:.4f}')\n",
    "print(f'Recall (ROC): {recall_roc:.4f}')\n",
    "print(f'F1-score (ROC): {f1_roc:.4f}')\n",
    "print(f'TN (ROC): {tn_roc}, FP (ROC): {fp_roc}, FN (ROC): {fn_roc}, TP (ROC): {tp_roc}')\n",
    "print()\n",
    "\n",
    "# Ottimizzazione del threshold utilizzando la curva precision-recall\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, reconstruction_loss)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_threshold_pr = thresholds_pr[np.argmax(f1_scores)]\n",
    "\n",
    "# Applica il threshold ottimizzato dalla curva precision-recall\n",
    "predictions_pr = (reconstruction_loss > optimal_threshold_pr).astype(np.int32)\n",
    "\n",
    "# Calcola le metriche utilizzando l'optimal_threshold_pr\n",
    "auc_pr, accuracy_pr, precision_pr, recall_pr, f1_pr, tn_pr, fp_pr, fn_pr, tp_pr = calculate_metrics(y_test, predictions_pr, reconstruction_loss)\n",
    "\n",
    "print(f'Precision-Recall Curve Optimization:')\n",
    "print(f'Optimal Threshold (PR): {optimal_threshold_pr:.4f}')\n",
    "print(f'AUC (PR): {auc_pr:.4f}')\n",
    "print(f'Accuracy (PR): {accuracy_pr:.4f}')\n",
    "print(f'Precision (PR): {precision_pr:.4f}')\n",
    "print(f'Recall (PR): {recall_pr:.4f}')\n",
    "print(f'F1-score (PR): {f1_pr:.4f}')\n",
    "print(f'TN (PR): {tn_pr}, FP (PR): {fp_pr}, FN (PR): {fn_pr}, TP (PR): {tp_pr}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
