{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/24j7jy9s6sb2xm_dc84kdhm00000gn/T/ipykernel_81222/1651886768.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "2024-07-05 15:50:00.230317: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/edoardozappia/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/edoardozappia/Downloads/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = df[df.Class == 1]\n",
    "normal = df[df.Class == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(['Time'], axis=1)\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227451, 29)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "X_train = X_train[X_train.Class == 0]\n",
    "X_train = X_train.drop(['Class'], axis=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "#y_test = X_test['Class']\n",
    "#X_test = X_test.drop(['Class'], axis=1)\n",
    "\n",
    "X_train = X_train.values\n",
    "#X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "class_0 = X_test[X_test.Class == 0]\n",
    "class_1 = X_test[X_test.Class == 1]\n",
    "\n",
    "total_size = len(X_test)\n",
    "desired_size_class_1 = int(total_size * 0.4)\n",
    "desired_size_class_0 = total_size - desired_size_class_1\n",
    "\n",
    "class_1_sampled = class_1.sample(n=desired_size_class_1, replace=True, random_state=42)\n",
    "class_0_sampled = class_0.sample(n=desired_size_class_0, replace=True, random_state=42)\n",
    "\n",
    "balanced_data = pd.concat([class_0_sampled, class_1_sampled])\n",
    "\n",
    "X_test = balanced_data.drop('Class', axis=1)\n",
    "y_test = balanced_data['Class']\n",
    "\n",
    "X_test = X_test.values\n",
    "y_test = y_test.values\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34178\n",
      "22784\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero(y_test == 0))\n",
    "print(np.count_nonzero(y_test == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "input_shape=(29,)\n",
    "print(input_dim)\n",
    "original_dim = input_dim  # Esempio di dimensione dell'input originale\n",
    "intermediary_dims = [20, 10 ,8]\n",
    "latent_dim = 2  # Esempio di dimensione dello spazio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:47:58.148118: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/edoardozappia/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 1.2974\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - loss: 1.2965\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step - loss: 1.2956\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 553ms/step - loss: 1.2946\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - loss: 1.2936\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 1.2926\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - loss: 1.2915\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - loss: 1.2903\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - loss: 1.2891\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 338ms/step - loss: 1.2879\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x145c51c40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "\n",
    "# Definizione della classe MultivariateNormalTriLLayer\n",
    "class MultivariateNormalTriLLayer(tfkl.Layer):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(MultivariateNormalTriLLayer, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.untransformed_scale_tril = self.add_weight(name='untransformed_scale_tril',\n",
    "                                                        shape=(self.latent_dim * (self.latent_dim + 1) // 2,),\n",
    "                                                        initializer='random_normal',\n",
    "                                                        trainable=True)\n",
    "        super(MultivariateNormalTriLLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        loc = inputs[..., :self.latent_dim]\n",
    "        scale_tril = tfp.math.fill_triangular(self.untransformed_scale_tril)\n",
    "        dist = tfd.MultivariateNormalTriL(loc=loc, scale_tril=scale_tril)\n",
    "        sample = dist.sample()\n",
    "        return sample  # Return the sample, not the distribution object\n",
    "\n",
    "# Definizione della funzione dense_layers\n",
    "def dense_layers(intermediary_dims):\n",
    "    return tfk.Sequential([\n",
    "        tfkl.Dense(units, activation='relu') for units in intermediary_dims\n",
    "    ])\n",
    "\n",
    "# Definizione del decoder come una classe Layer\n",
    "class Decoder(tfkl.Layer):\n",
    "    def __init__(self, original_dim, intermediary_dims, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.decoder_hidden = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation='relu') for units in intermediary_dims\n",
    "        ] + [\n",
    "            tf.keras.layers.Dense(original_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.decoder_hidden(inputs)\n",
    "\n",
    "# Definizione del modello VAE\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, original_dim, intermediary_dims, latent_dim, prior, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediary_dims = intermediary_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior = prior\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(original_dim, intermediary_dims)\n",
    "\n",
    "    def build_encoder(self):\n",
    "        x = dense_layers(self.intermediary_dims)(self.encoder_inputs)\n",
    "        params = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(self.latent_dim), activation=None)(x)\n",
    "        z = MultivariateNormalTriLLayer(self.latent_dim)(params)\n",
    "        return tf.keras.Model(self.encoder_inputs, z, name='encoder')\n",
    "\n",
    "    def kl_divergence(self, distribution_a):\n",
    "        distribution_b = self.prior\n",
    "        return tfpl.KLDivergenceAddLoss(\n",
    "            tfpl.MultivariateNormalTriL(),\n",
    "            distribution_b,\n",
    "            weight=1.0\n",
    "        )(distribution_a)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_sample = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z_sample)\n",
    "        return reconstructed\n",
    "\n",
    "# Definizione dei parametri\n",
    "input_dim = X_train.shape[1]\n",
    "original_dim = input_dim\n",
    "intermediary_dims = [20, 10, 8]\n",
    "latent_dim = 2\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1),\n",
    "                        reinterpreted_batch_ndims=1)\n",
    "\n",
    "# Creazione dell'istanza del modello VAE\n",
    "vae = VAE(original_dim, intermediary_dims, latent_dim, prior)\n",
    "\n",
    "# Compilazione del modello specificando la loss\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam(), loss=reconstruction_loss)\n",
    "\n",
    "# Allenamento del modello\n",
    "vae.fit(X_train, X_train, epochs=10, batch_size=2000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Tensor(\"mul:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edoardozappia/Library/Python/3.9/lib/python/site-packages/keras/src/optimizers/base_optimizer.py:664: UserWarning: Gradients do not exist for variables ['kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias', 'kernel', 'bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul:0\", shape=(), dtype=float32)\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - kl_divergence: 0.9759 - loss: 38.7745 - reconstruction_loss: 37.7986\n",
      "Epoch 2/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - kl_divergence: 0.5966 - loss: 38.3366 - reconstruction_loss: 37.7400\n",
      "Epoch 3/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - kl_divergence: 0.4606 - loss: 38.1971 - reconstruction_loss: 37.7366\n",
      "Epoch 4/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - kl_divergence: 0.3168 - loss: 38.0462 - reconstruction_loss: 37.7294\n",
      "Epoch 5/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - kl_divergence: 0.1974 - loss: 37.9245 - reconstruction_loss: 37.7271\n",
      "Epoch 6/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - kl_divergence: 0.1837 - loss: 37.9165 - reconstruction_loss: 37.7328\n",
      "Epoch 7/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - kl_divergence: 0.1713 - loss: 37.8986 - reconstruction_loss: 37.7273\n",
      "Epoch 8/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - kl_divergence: 0.1094 - loss: 37.8365 - reconstruction_loss: 37.7271\n",
      "Epoch 9/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - kl_divergence: 0.0618 - loss: 37.7883 - reconstruction_loss: 37.7264\n",
      "Epoch 10/10\n",
      "\u001b[1m889/889\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - kl_divergence: 0.0379 - loss: 37.7653 - reconstruction_loss: 37.7274\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1434a1580>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "\n",
    "# Definizione della classe MultivariateNormalTriLLayer\n",
    "class MultivariateNormalTriLLayer(tfkl.Layer):\n",
    "    def __init__(self, latent_dim, **kwargs):\n",
    "        super(MultivariateNormalTriLLayer, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.untransformed_scale_tril = self.add_weight(name='untransformed_scale_tril',\n",
    "                                                        shape=(self.latent_dim * (self.latent_dim + 1) // 2,),\n",
    "                                                        initializer='random_normal',\n",
    "                                                        trainable=True)\n",
    "        super(MultivariateNormalTriLLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        loc = inputs[..., :self.latent_dim]\n",
    "        scale_tril = tfp.math.fill_triangular(self.untransformed_scale_tril)\n",
    "        dist = tfd.MultivariateNormalTriL(loc=loc, scale_tril=scale_tril)\n",
    "        sample = dist.sample()\n",
    "        return sample  # Return the sample, not the distribution object\n",
    "\n",
    "# Definizione della funzione dense_layers\n",
    "def dense_layers(intermediary_dims):\n",
    "    return tfk.Sequential([\n",
    "        tfkl.Dense(units, activation='relu') for units in intermediary_dims\n",
    "    ])\n",
    "\n",
    "# Definizione del decoder come una classe Layer\n",
    "class Decoder(tfkl.Layer):\n",
    "    def __init__(self, original_dim, intermediary_dims, name='decoder', **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.decoder_hidden = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(units, activation='relu') for units in intermediary_dims\n",
    "        ] + [\n",
    "            tf.keras.layers.Dense(original_dim, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.decoder_hidden(inputs)\n",
    "\n",
    "# Definizione del modello VAE\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, original_dim, intermediary_dims, latent_dim, prior, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.intermediary_dims = intermediary_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        self.prior = prior\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "        self.encoder = self.build_encoder()\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(original_dim, intermediary_dims)\n",
    "\n",
    "    def build_encoder(self):\n",
    "        x = dense_layers(self.intermediary_dims)(self.encoder_inputs)\n",
    "        params = tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(self.latent_dim), activation=None)(x)\n",
    "        z = MultivariateNormalTriLLayer(self.latent_dim)(params)\n",
    "        return tf.keras.Model(self.encoder_inputs, [z, params], name='encoder')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        if isinstance(inputs, tuple):\n",
    "            inputs = inputs[0]\n",
    "        z_sample, _ = self.encoder(inputs)  # Otteniamo sia z_sample che params\n",
    "        reconstructed = self.decoder(z_sample)\n",
    "        return reconstructed\n",
    "\n",
    "    def compute_kl_divergence(self, q_z_x_params):\n",
    "        q_distribution = tfd.MultivariateNormalTriL(loc=q_z_x_params[..., :self.latent_dim],\n",
    "                                                    scale_tril=tfp.math.fill_triangular(q_z_x_params[..., self.latent_dim:]))\n",
    "        p_distribution = self.prior\n",
    "        return tfd.kl_divergence(q_distribution, p_distribution)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]  # Assuming data is a tuple of (input, target), take the input\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = self(data)\n",
    "            reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "            reconstruction_loss = reconstruction_loss_fn(data, reconstructed)\n",
    "            reconstruction_loss *= self.original_dim\n",
    "            print(reconstruction_loss)\n",
    "            z_sample, q_z_x_params = self.encoder(data)\n",
    "            kl_divergence = tf.reduce_mean(self.compute_kl_divergence(q_z_x_params))\n",
    "\n",
    "            elbo_loss = reconstruction_loss + kl_divergence\n",
    "        gradients = tape.gradient(elbo_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        return {\"loss\": elbo_loss, \"reconstruction_loss\": reconstruction_loss, \"kl_divergence\": kl_divergence}\n",
    "\n",
    "# Definizione dei parametri\n",
    "input_dim = X_train.shape[1]\n",
    "original_dim = input_dim\n",
    "intermediary_dims = [20, 10, 8]\n",
    "latent_dim = 2\n",
    "prior = tfd.MultivariateNormalDiag(loc=tf.zeros(latent_dim), scale_diag=tf.ones(latent_dim))\n",
    "\n",
    "# Creazione dell'istanza del modello VAE\n",
    "vae = VAE(original_dim, intermediary_dims, latent_dim, prior)\n",
    "\n",
    "# Compilazione del modello specificando la loss\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Allenamento del modello\n",
    "vae.fit(X_train, X_train, epochs=10, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supponendo che vae sia il tuo modello VAE addestrato e X_test e y_test siano i tuoi dati di test\n",
    "#X_test_encoded = vae.encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della perdita di ricostruzione su X_test\n",
    "#reconstructed = vae.decoder(vae.encoder(X_test)).numpy()\n",
    "#reconstruction_loss = tf.reduce_mean(tf.keras.losses.mean_squared_error(X_test, reconstructed), axis=1)\n",
    "\n",
    "# Definizione di una soglia arbitraria (può essere ottimizzata)\n",
    "#threshold = 2.5  # Esempio di soglia, da ottimizzare\n",
    "\n",
    "# Creazione di un array binario per le previsioni basate sulla soglia\n",
    "#predictions = reconstruction_loss > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo della perdita di ricostruzione su X_test\n",
    "reconstructed = vae.decoder(vae.encoder(X_test)).numpy()\n",
    "reconstruction_loss = np.mean(np.square(X_test - reconstructed), axis=1)\n",
    "\n",
    "# Gestione dei NaN\n",
    "nan_indices = np.isnan(reconstruction_loss)\n",
    "if np.any(nan_indices):\n",
    "    # Sostituisci i valori NaN con un valore arbitrario (ad esempio, 0)\n",
    "    reconstruction_loss[nan_indices] = 0  # Puoi scegliere un valore appropriato\n",
    "\n",
    "# Definizione di una soglia arbitraria (può essere ottimizzata)\n",
    "threshold = 2.5  # Esempio di soglia, da ottimizzare\n",
    "\n",
    "# Creazione di un array binario per le previsioni basate sulla soglia\n",
    "predictions = (reconstruction_loss > threshold).astype(np.int32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.62549618   0.68555047   1.42522673 ...   1.34832502   3.68736853\n",
      " 110.26447136]\n"
     ]
    }
   ],
   "source": [
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metriche con soglia arbitraria:\n",
      "AUC: 0.9526\n",
      "Accuracy: 0.9111\n",
      "Precision: 0.9192\n",
      "Recall: 0.8528\n",
      "F1-score: 0.8848\n",
      "TN: 32469, FP: 1709, FN: 3353, TP: 19431\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/edoardozappia/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC Curve Optimization:\n",
      "Optimal Threshold (ROC): inf\n",
      "AUC (ROC): 0.9526\n",
      "Accuracy (ROC): 0.6000\n",
      "Precision (ROC): 0.0000\n",
      "Recall (ROC): 0.0000\n",
      "F1-score (ROC): 0.0000\n",
      "TN (ROC): 34178, FP (ROC): 0, FN (ROC): 22784, TP (ROC): 0\n",
      "\n",
      "Precision-Recall Curve Optimization:\n",
      "Optimal Threshold (PR): 2.3633\n",
      "AUC (PR): 0.9526\n",
      "Accuracy (PR): 0.9261\n",
      "Precision (PR): 0.9165\n",
      "Recall (PR): 0.8971\n",
      "F1-score (PR): 0.9067\n",
      "TN (PR): 32316, FP (PR): 1862, FN (PR): 2345, TP (PR): 20439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definisci la funzione per calcolare le metriche in base al threshold\n",
    "def calculate_metrics(y_test, predictions, reconstruction_loss):\n",
    "    # Calcolo delle metriche\n",
    "    auc = roc_auc_score(y_test, reconstruction_loss)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return auc, accuracy, precision, recall, f1, tn, fp, fn, tp\n",
    "\n",
    "nan_indices = np.isnan(reconstruction_loss)\n",
    "if np.any(nan_indices):\n",
    "    print(f\"Found NaN values in reconstruction_loss at indices: {np.where(nan_indices)}\")\n",
    "\n",
    "# Calcolo delle metriche per la soglia arbitraria\n",
    "auc, accuracy, precision, recall, f1, tn, fp, fn, tp = calculate_metrics(y_test, predictions, reconstruction_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f'Metriche con soglia arbitraria:')\n",
    "print(f'AUC: {auc:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
    "print()\n",
    "\n",
    "# Ottimizzazione del threshold utilizzando la curva ROC\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, reconstruction_loss)\n",
    "roc_auc_scores = [roc_auc_score(y_test, reconstruction_loss) for threshold in thresholds_roc]\n",
    "optimal_threshold_roc = thresholds_roc[np.argmax(roc_auc_scores)]\n",
    "\n",
    "# Applica il threshold ottimizzato dalla curva ROC\n",
    "predictions_roc = (reconstruction_loss > optimal_threshold_roc).astype(np.int32)\n",
    "\n",
    "# Calcola le metriche utilizzando l'optimal_threshold_roc\n",
    "auc_roc, accuracy_roc, precision_roc, recall_roc, f1_roc, tn_roc, fp_roc, fn_roc, tp_roc = calculate_metrics(y_test, predictions_roc, reconstruction_loss)\n",
    "\n",
    "print(f'ROC Curve Optimization:')\n",
    "print(f'Optimal Threshold (ROC): {optimal_threshold_roc:.4f}')\n",
    "print(f'AUC (ROC): {auc_roc:.4f}')\n",
    "print(f'Accuracy (ROC): {accuracy_roc:.4f}')\n",
    "print(f'Precision (ROC): {precision_roc:.4f}')\n",
    "print(f'Recall (ROC): {recall_roc:.4f}')\n",
    "print(f'F1-score (ROC): {f1_roc:.4f}')\n",
    "print(f'TN (ROC): {tn_roc}, FP (ROC): {fp_roc}, FN (ROC): {fn_roc}, TP (ROC): {tp_roc}')\n",
    "print()\n",
    "\n",
    "# Ottimizzazione del threshold utilizzando la curva precision-recall\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, reconstruction_loss)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_threshold_pr = thresholds_pr[np.argmax(f1_scores)]\n",
    "\n",
    "# Applica il threshold ottimizzato dalla curva precision-recall\n",
    "predictions_pr = (reconstruction_loss > optimal_threshold_pr).astype(np.int32)\n",
    "\n",
    "# Calcola le metriche utilizzando l'optimal_threshold_pr\n",
    "auc_pr, accuracy_pr, precision_pr, recall_pr, f1_pr, tn_pr, fp_pr, fn_pr, tp_pr = calculate_metrics(y_test, predictions_pr, reconstruction_loss)\n",
    "\n",
    "print(f'Precision-Recall Curve Optimization:')\n",
    "print(f'Optimal Threshold (PR): {optimal_threshold_pr:.4f}')\n",
    "print(f'AUC (PR): {auc_pr:.4f}')\n",
    "print(f'Accuracy (PR): {accuracy_pr:.4f}')\n",
    "print(f'Precision (PR): {precision_pr:.4f}')\n",
    "print(f'Recall (PR): {recall_pr:.4f}')\n",
    "print(f'F1-score (PR): {f1_pr:.4f}')\n",
    "print(f'TN (PR): {tn_pr}, FP (PR): {fp_pr}, FN (PR): {fn_pr}, TP (PR): {tp_pr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m auc, accuracy, precision, recall, f1, tn, fp, fn, tp\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calcolo delle metriche per la soglia arbitraria\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m auc, accuracy, precision, recall, f1, tn, fp, fn, tp \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstruction_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetriche con soglia arbitraria:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(y_test, predictions, reconstruction_loss)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_metrics\u001b[39m(y_test, predictions, reconstruction_loss):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Calcolo delle metriche\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     auc \u001b[38;5;241m=\u001b[39m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstruction_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, predictions)\n\u001b[1;32m     16\u001b[0m     precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, predictions)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_ranking.py:606\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    604\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 606\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    609\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    610\u001b[0m ):\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "# Calcolo della perdita di ricostruzione su X_test\n",
    "reconstructed = vae.decoder(vae.encoder(X_test)).numpy()\n",
    "reconstruction_loss = np.mean(np.square(X_test - reconstructed), axis=1)\n",
    "\n",
    "# Definizione di una soglia arbitraria (può essere ottimizzata)\n",
    "threshold = 2.5  # Esempio di soglia, da ottimizzare\n",
    "\n",
    "# Creazione di un array binario per le previsioni basate sulla soglia\n",
    "predictions = (reconstruction_loss > threshold).astype(np.int32)\n",
    "\n",
    "# Definisci la funzione per calcolare le metriche in base al threshold\n",
    "def calculate_metrics(y_test, predictions, reconstruction_loss):\n",
    "    # Calcolo delle metriche\n",
    "    auc = roc_auc_score(y_test, reconstruction_loss)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return auc, accuracy, precision, recall, f1, tn, fp, fn, tp\n",
    "\n",
    "# Calcolo delle metriche per la soglia arbitraria\n",
    "auc, accuracy, precision, recall, f1, tn, fp, fn, tp = calculate_metrics(y_test, predictions, reconstruction_loss)\n",
    "\n",
    "print(f'Metriche con soglia arbitraria:')\n",
    "print(f'AUC: {auc:.4f}')\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1-score: {f1:.4f}')\n",
    "print(f'TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}')\n",
    "print()\n",
    "\n",
    "# Ottimizzazione del threshold utilizzando la curva ROC\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_test, reconstruction_loss)\n",
    "roc_auc_scores = [roc_auc_score(y_test, reconstruction_loss) for threshold in thresholds_roc]\n",
    "optimal_threshold_roc = thresholds_roc[np.argmax(roc_auc_scores)]\n",
    "\n",
    "# Applica il threshold ottimizzato dalla curva ROC\n",
    "predictions_roc = (reconstruction_loss > optimal_threshold_roc).astype(np.int32)\n",
    "\n",
    "# Calcola le metriche utilizzando l'optimal_threshold_roc\n",
    "auc_roc, accuracy_roc, precision_roc, recall_roc, f1_roc, tn_roc, fp_roc, fn_roc, tp_roc = calculate_metrics(y_test, predictions_roc, reconstruction_loss)\n",
    "\n",
    "print(f'ROC Curve Optimization:')\n",
    "print(f'Optimal Threshold (ROC): {optimal_threshold_roc:.4f}')\n",
    "print(f'AUC (ROC): {auc_roc:.4f}')\n",
    "print(f'Accuracy (ROC): {accuracy_roc:.4f}')\n",
    "print(f'Precision (ROC): {precision_roc:.4f}')\n",
    "print(f'Recall (ROC): {recall_roc:.4f}')\n",
    "print(f'F1-score (ROC): {f1_roc:.4f}')\n",
    "print(f'TN (ROC): {tn_roc}, FP (ROC): {fp_roc}, FN (ROC): {fn_roc}, TP (ROC): {tp_roc}')\n",
    "print()\n",
    "\n",
    "# Ottimizzazione del threshold utilizzando la curva precision-recall\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_test, reconstruction_loss)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "optimal_threshold_pr = thresholds_pr[np.argmax(f1_scores)]\n",
    "\n",
    "# Applica il threshold ottimizzato dalla curva precision-recall\n",
    "predictions_pr = (reconstruction_loss > optimal_threshold_pr).astype(np.int32)\n",
    "\n",
    "# Calcola le metriche utilizzando l'optimal_threshold_pr\n",
    "auc_pr, accuracy_pr, precision_pr, recall_pr, f1_pr, tn_pr, fp_pr, fn_pr, tp_pr = calculate_metrics(y_test, predictions_pr, reconstruction_loss)\n",
    "\n",
    "print(f'Precision-Recall Curve Optimization:')\n",
    "print(f'Optimal Threshold (PR): {optimal_threshold_pr:.4f}')\n",
    "print(f'AUC (PR): {auc_pr:.4f}')\n",
    "print(f'Accuracy (PR): {accuracy_pr:.4f}')\n",
    "print(f'Precision (PR): {precision_pr:.4f}')\n",
    "print(f'Recall (PR): {recall_pr:.4f}')\n",
    "print(f'F1-score (PR): {f1_pr:.4f}')\n",
    "print(f'TN (PR): {tn_pr}, FP (PR): {fp_pr}, FN (PR): {fn_pr}, TP (PR): {tp_pr}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
